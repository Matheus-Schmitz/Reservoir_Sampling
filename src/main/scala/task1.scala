// DSCI 553 | Foundations and Applications of Data Mining
// Homework 4
// Matheus Schmitz
// USC ID: 5039286453

import org.apache.spark.rdd.RDD
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import scala.collection.mutable
import java.io._
import scala.io.Source
import scala.util.Random


object task1 {

  // Generate parameters for the hash functions
  val hash_params: mutable.ListBuffer[(Int, Int)] = mutable.ListBuffer.empty[(Int, Int)]
  val random_gen: Random.type = scala.util.Random
  for (a <- 0 until 1){
    var hashes = (random_gen.nextInt(100), random_gen.nextInt(100))
    hash_params.append(hashes)
  }

  // Initialize the filter bit array
  var filter_bit_array: mutable.ListBuffer[Int] = mutable.ListBuffer.empty[Int]
  for (a <- 0 until 69997){
    filter_bit_array.append(0)
  }
  var m: Int = filter_bit_array.length


  def myhashs(user: String): mutable.ListBuffer[Int] = {

    // Encode user to int
    val user_int = user.hashCode  //BigInt(user.getBytes()).toInt  //user.map(_.asDigit.toString).mkString.toInt

    // Generate hash values
    val results: mutable.ListBuffer[Int] = mutable.ListBuffer.empty[Int]
    for (f <- hash_params){
      results.append(((f._1 * user_int + f._2) % m).abs)
    }
    return results
  }

  def main(args: Array[String]): Unit = {

    // Read user inputs
    val input_filename = args(0)
    val stream_size = args(1).toInt
    val num_of_asks = args(2).toInt
    val output_filename = args(3)
    //val input_filename = "publicdata/users.txt"
    //val stream_size = 300
    //val num_of_asks = 300
    //val output_filename = "scala1.csv"

    // Initialize Spark with the 4 GB memory parameters from HW1
    val config = new SparkConf().setMaster("local[*]").setAppName("task1").set("spark.executor.memory", "4g").set("spark.driver.memory", "4g")//.set("spark.testing.memory", "471859200")
    val sc = SparkContext.getOrCreate(config)
    sc.setLogLevel("ERROR")

    // Variables to track performance
    var seen_users_truth: mutable.Set[String] = mutable.Set.empty[String]
    var seen_users_preds: mutable.Set[String] = mutable.Set.empty[String]

    // Before beginning to iterate, write the column headers
    val pw = new PrintWriter(new File(output_filename))
    pw.write("Time,FPR")

    // Blackbox
    val BB = Blackbox()

    // Iterate over the asks
    for (ask_iteration <- 0 until num_of_asks) {
      var stream_users = BB.ask(input_filename, stream_size)

      // According to Piazza post 502 FP and TN should be reset between iterations
      var FP = 0
      var TN = 0

      // Scala requires "stream_hashed_idxs" to be first created outside the loop so it can later be accessed out the loop for updating the filter_bit_array
      var stream_hashed_idxs: mutable.ListBuffer[mutable.ListBuffer[Int]] = mutable.ListBuffer.empty[mutable.ListBuffer[Int]]

      // Go over all users for this stream
      for (user <- stream_users) {

        // List to store all indexes generated by all hashes * users for this iteration, so the bit array can be updated when the iteration ends
        // Clear the list buffer between iterations
        stream_hashed_idxs.clear()

        // If I have actually seen the user before, then I should not test the match, as surely it will match because it should
        // In this case it would be a true positive, which I'm not interested in
        if (!seen_users_truth.contains(user)) {

          // Hash the user into values
          var hashed_idxs = myhashs(user)

          // Track how many of the hashed indexes collide with pre-existing 1s on the filter_bit_array
          var bit_matches = 0
          for (idx <- hashed_idxs) {
            if (filter_bit_array(idx) == 1) {
              bit_matches += 1
            }
          }

          // If all bits matches, then we have a false positive, else a true negative
          if (bit_matches == hashed_idxs.length) {

            // Make a prediction that already seen the users
            seen_users_preds.add(user)

            // Since the truly seen users have been weed out before, I know this is a false positive
            FP += 1
          }
          else {
            TN += 1
          }
          
          // Add the current user hashed indexes to the list with all indexes from the iteration, so that when the iteration finishes the bit array can be updated
          stream_hashed_idxs.append(hashed_idxs)
        }
      }
      // Update the Bloom Filter after the iteration is done
      for (user_hashed_idxs <- stream_hashed_idxs){
        for (idx <- user_hashed_idxs){
          filter_bit_array(idx) = 1
        }
      }
      // Update the ground truth
      seen_users_truth ++= stream_users

      // Once all users of the current stream have been iterated through, calculate the iteration's FPR
      var FPR = FP.toFloat / (FP.toFloat + TN.toFloat)

      // Then append the results to the output file
      pw.write("\n" + ask_iteration + "," + FPR)
    }
    pw.close()
    sc.stop()
  }
}
